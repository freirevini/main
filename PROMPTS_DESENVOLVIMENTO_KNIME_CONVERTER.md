# üìã PROMPTS DE DESENVOLVIMENTO - KNIME TO PYTHON CONVERTER

## Guia de Prompts para Implementa√ß√£o Etapa por Etapa

**Ferramenta:** Windsurf com Claude Opus 4.5  
**M√©todo:** Enviar um prompt por vez, validar resultado, prosseguir para pr√≥ximo  
**Data:** 2025-12-15

---

## üî¥ PROMPT ZERO - EXTRA√á√ÉO DO PROJETO LEGADO

> **OBJETIVO:** Extrair conhecimento do c√≥digo legado antes de come√ßar o novo projeto.  
> **QUANDO USAR:** Execute este prompt PRIMEIRO, em um chat separado, apontando para o arquivo do projeto legado.

```
Analise o arquivo Python anexado que √© um conversor KNIME para Python (vers√£o legada).

Preciso que voc√™ extraia e organize as seguintes informa√ß√µes em formato JSON estruturado:

## 1. MAPEAMENTO DE NODES

Para cada tipo de node que o c√≥digo processa, extraia:
- `factory`: classe Java completa (ex: org.knime.base.node.io.csvreader.CSVReaderNodeFactory)
- `name`: nome leg√≠vel do node
- `category`: categoria (io, manipulation, transformation, logic, flow_control, database)
- `settings_extraction`: quais campos do settings.xml s√£o lidos e como (xpath ou chave)
- `python_template`: c√≥digo Python gerado para este node (template com placeholders)
- `imports`: bibliotecas necess√°rias

## 2. IDENTIFICA√á√ÉO DE METANODES

Extraia a l√≥gica usada para:
- Como identificar se uma pasta √© um metanode
- Como acessar o workflow.knime interno do metanode
- Como tratar vari√°veis expostas pelo metanode
- Padr√µes de nome que indicam metanodes especiais (ex: "CRIA DATA")

## 3. CONEX√ïES ENTRE NODES

Extraia a l√≥gica de:
- Como o c√≥digo l√™ as conex√µes do workflow.knime
- Estrutura de dados usada para armazenar conex√µes (source_id, dest_id, ports)
- Como determinar a ordem de execu√ß√£o (algoritmo de ordena√ß√£o topol√≥gica)
- Como tratar m√∫ltiplas entradas em um node (joins, concatenates)

## 4. CONEX√ïES DE BANCO DE DADOS

Extraia informa√ß√µes sobre:
- Nodes de conex√£o suportados (Database Connector, BigQuery, etc.)
- Como extrair JDBC URLs e credenciais
- Como mapear queries SQL para pandas
- Tratamento de vari√°veis de fluxo em queries (${variable})

## 5. MAPEAMENTO DE TIPOS

Extraia o dicion√°rio completo de:
- Tipos KNIME (cell classes) ‚Üí tipos Python/Pandas
- Tratamento de tipos especiais (datas, cole√ß√µes, blobs)

## 6. PADR√ïES DE EXPRESS√ïES

Extraia mapeamentos de:
- Fun√ß√µes matem√°ticas KNIME ‚Üí NumPy (pow, sqrt, etc.)
- Sintaxe de refer√™ncia a colunas ($coluna$)
- Operadores de compara√ß√£o e l√≥gicos

## FORMATO DE SA√çDA

Gere 3 arquivos JSON:

### node_mapping_extracted.json
{
  "version": "extracted_from_legacy",
  "extraction_date": "2025-12-15",
  "mappings": [
    {
      "factory": "...",
      "name": "...",
      "category": "...",
      "settings_extraction": {},
      "python_template": "...",
      "imports": []
    }
  ]
}

### knime_types_extracted.json
{
  "type_mappings": {
    "org.knime.core.data.def.DoubleCell": "Float64",
    ...
  },
  "expression_mappings": {
    "pow": "np.power",
    ...
  },
  "column_reference_pattern": "\\$([^$]+)\\$"
}

### connection_patterns_extracted.json
{
  "metanode_detection": {
    "indicators": ["..."],
    "special_patterns": ["CRIA DATA", ...]
  },
  "edge_extraction": {
    "xml_path": "...",
    "structure": {}
  },
  "database_nodes": [
    {
      "factory": "...",
      "connection_extraction": {}
    }
  ]
}

Seja exaustivo - extraia TODOS os nodes, tipos e padr√µes que encontrar no c√≥digo.
Para cada item, inclua um coment√°rio sobre onde no c√≥digo legado est√° a implementa√ß√£o (n√∫mero da linha aproximado ou nome da fun√ß√£o).
```

---

## üü¢ SPRINT 1: FUNDA√á√ÉO (Extra√ß√£o)

### PROMPT 1.1 - Estrutura do Projeto e Extrator ZIP

```
Crie a estrutura inicial de um projeto Python para processamento de arquivos.

## ESTRUTURA DE DIRET√ìRIOS

Crie a seguinte estrutura:

knime_converter/
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ output/
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ extractors/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ zip_extractor.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ logger.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ test_zip_extractor.py
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md

## ARQUIVO: src/utils/logger.py

Crie um m√≥dulo de logging com:
- Configura√ß√£o de logging para console e arquivo
- N√≠veis: DEBUG, INFO, WARNING, ERROR
- Formato: timestamp | n√≠vel | m√≥dulo | mensagem
- Arquivo de log em: output/converter.log

## ARQUIVO: src/extractors/zip_extractor.py

Crie uma classe `KnimeZipExtractor` com:

```python
class KnimeZipExtractor:
    """
    Extrai conte√∫do de arquivos .knwf (KNIME Workflow).
    Arquivos .knwf s√£o ZIPs renomeados contendo a estrutura do workflow.
    """
    
    def __init__(self, file_path: str):
        """
        Inicializa o extrator.
        
        Args:
            file_path: Caminho para arquivo .knwf ou pasta j√° extra√≠da
        """
        pass
    
    def extract(self, output_dir: str = None) -> Path:
        """
        Extrai o arquivo .knwf para uma pasta tempor√°ria.
        
        Args:
            output_dir: Diret√≥rio de destino (opcional, usa tempdir se None)
            
        Returns:
            Path para a pasta extra√≠da
            
        Raises:
            FileNotFoundError: Se arquivo n√£o existe
            zipfile.BadZipFile: Se n√£o √© um ZIP v√°lido
        """
        pass
    
    def get_workflow_structure(self) -> dict:
        """
        Retorna a estrutura de arquivos do workflow.
        
        Returns:
            Dict com:
            - root_path: caminho raiz
            - workflow_file: caminho do workflow.knime
            - node_folders: lista de pastas de nodes
            - has_metanodes: bool indicando presen√ßa de metanodes
        """
        pass
    
    def cleanup(self):
        """Remove pasta tempor√°ria se foi criada."""
        pass
```

## REQUISITOS T√âCNICOS

1. Usar `pathlib.Path` para todos os caminhos
2. Usar `tempfile.mkdtemp()` para pasta tempor√°ria
3. Validar se o arquivo √© um ZIP v√°lido antes de extrair
4. Logar cada etapa do processo
5. Tratar arquivos .knwf e pastas j√° extra√≠das
6. Identificar pastas de nodes pelo padr√£o: nome contendo "(#N)" onde N √© n√∫mero

## ARQUIVO: tests/test_zip_extractor.py

Crie testes unit√°rios com pytest:
- test_init_with_valid_file
- test_init_with_invalid_file
- test_init_with_folder
- test_extract_creates_folder
- test_get_workflow_structure_returns_dict
- test_cleanup_removes_temp_folder

## ARQUIVO: requirements.txt

```
pytest>=7.0.0
pathlib>=1.0.1
```

## ARQUIVO: README.md

Crie documenta√ß√£o b√°sica com:
- Nome do projeto
- Descri√ß√£o breve
- Como instalar depend√™ncias
- Como rodar testes

Implemente todos os arquivos com c√≥digo completo e funcional.
Adicione docstrings detalhadas e coment√°rios explicando a l√≥gica.
Use type hints em todas as fun√ß√µes.
```

---

### PROMPT 1.2 - Parser do workflow.knime

```
Adicione ao projeto existente um parser para o arquivo workflow.knime.

## ARQUIVO: src/extractors/workflow_parser.py

Crie uma classe `WorkflowParser` que l√™ o arquivo XML workflow.knime e extrai nodes e conex√µes.

### CONTEXTO T√âCNICO DO XML

O arquivo workflow.knime usa namespace: "http://www.knime.org/2008/09/XMLConfig"

Estrutura relevante:
- Nodes est√£o em: <config key="nodes"><config key="node_N">...</config></config>
- Conex√µes est√£o em: <config key="connections"><config key="connection_N">...</config></config>

Cada node tem:
- <entry key="id" type="xint" value="N"/>
- <entry key="node_settings_file" type="xstring" value="Nome do Node (#N)/settings.xml"/>
- <entry key="node_is_meta" type="xboolean" value="true/false"/>

Cada conex√£o tem:
- <entry key="sourceID" type="xint" value="N"/>
- <entry key="destID" type="xint" value="N"/>
- <entry key="sourcePort" type="xint" value="N"/>
- <entry key="destPort" type="xint" value="N"/>

### IMPLEMENTA√á√ÉO

```python
from dataclasses import dataclass
from typing import List, Dict, Optional
from pathlib import Path
import xml.etree.ElementTree as ET

@dataclass
class NodeInfo:
    """Informa√ß√µes b√°sicas de um node extra√≠das do workflow.knime."""
    id: str
    display_name: str
    settings_path: str
    is_metanode: bool
    position_x: int = 0
    position_y: int = 0

@dataclass
class ConnectionInfo:
    """Informa√ß√£o de uma conex√£o entre nodes."""
    source_id: str
    dest_id: str
    source_port: int
    dest_port: int
    connection_type: str = "data"  # "data" ou "flow"

class WorkflowParser:
    """
    Parser para arquivo workflow.knime (XML).
    Extrai lista de nodes e conex√µes do workflow.
    """
    
    NAMESPACE = {"k": "http://www.knime.org/2008/09/XMLConfig"}
    
    def __init__(self, workflow_path: Path):
        """
        Args:
            workflow_path: Caminho para arquivo workflow.knime
        """
        pass
    
    def parse(self) -> Dict:
        """
        Faz o parsing completo do workflow.
        
        Returns:
            Dict com:
            - metadata: informa√ß√µes do workflow (autor, vers√£o, etc)
            - nodes: Dict[str, NodeInfo] mapeando id -> NodeInfo
            - connections: List[ConnectionInfo]
            - annotations: List[str] (textos de anota√ß√µes no workflow)
        """
        pass
    
    def _parse_with_namespace(self, root: ET.Element) -> Dict:
        """Tenta parsing com namespace KNIME."""
        pass
    
    def _parse_without_namespace(self, root: ET.Element) -> Dict:
        """Fallback: parsing sem namespace (vers√µes antigas)."""
        pass
    
    def _extract_nodes(self, nodes_config: ET.Element) -> Dict[str, NodeInfo]:
        """Extrai todos os nodes do elemento <config key="nodes">."""
        pass
    
    def _extract_connections(self, connections_config: ET.Element) -> List[ConnectionInfo]:
        """Extrai todas as conex√µes do elemento <config key="connections">."""
        pass
    
    def _extract_metadata(self, root: ET.Element) -> Dict:
        """Extrai metadados: autor, vers√£o, credenciais."""
        pass
    
    def get_node_by_id(self, node_id: str) -> Optional[NodeInfo]:
        """Retorna node pelo ID."""
        pass
    
    def get_predecessors(self, node_id: str) -> List[str]:
        """Retorna IDs dos nodes que enviam dados para este node."""
        pass
    
    def get_successors(self, node_id: str) -> List[str]:
        """Retorna IDs dos nodes que recebem dados deste node."""
        pass
```

### TRATAMENTO DE ERROS

1. Se arquivo n√£o existe: raise FileNotFoundError com mensagem clara
2. Se XML inv√°lido: raise ValueError("workflow.knime inv√°lido: {detalhes}")
3. Se namespace n√£o reconhecido: tentar sem namespace (fallback)
4. Logar warnings para elementos n√£o reconhecidos

### ARQUIVO: tests/test_workflow_parser.py

Crie testes com um XML de exemplo inline:
- test_parse_returns_dict_with_required_keys
- test_extract_nodes_returns_node_info
- test_extract_connections_returns_connection_info
- test_get_predecessors_returns_correct_ids
- test_get_successors_returns_correct_ids
- test_parse_without_namespace_fallback

### XML DE TESTE (use como fixture)

```xml
<?xml version="1.0" encoding="UTF-8"?>
<config xmlns="http://www.knime.org/2008/09/XMLConfig" key="workflow.knime">
    <entry key="created_by" type="xstring" value="4.5.2"/>
    <config key="nodes">
        <config key="node_1">
            <entry key="id" type="xint" value="1"/>
            <entry key="node_settings_file" type="xstring" value="CSV Reader (#1)/settings.xml"/>
            <entry key="node_is_meta" type="xboolean" value="false"/>
        </config>
        <config key="node_2">
            <entry key="id" type="xint" value="2"/>
            <entry key="node_settings_file" type="xstring" value="Column Filter (#2)/settings.xml"/>
            <entry key="node_is_meta" type="xboolean" value="false"/>
        </config>
    </config>
    <config key="connections">
        <config key="connection_0">
            <entry key="sourceID" type="xint" value="1"/>
            <entry key="destID" type="xint" value="2"/>
            <entry key="sourcePort" type="xint" value="1"/>
            <entry key="destPort" type="xint" value="1"/>
        </config>
    </config>
</config>
```

Implemente c√≥digo completo, funcional, com todos os m√©todos implementados.
```

---

### PROMPT 1.3 - Parser do settings.xml

```
Adicione ao projeto existente um parser para arquivos settings.xml dos nodes.

## ARQUIVO: src/extractors/settings_parser.py

Crie uma classe `SettingsParser` que extrai configura√ß√µes de cada node.

### CONTEXTO T√âCNICO

Cada node tem uma pasta com settings.xml contendo:
- `factory`: classe Java que identifica o tipo de node
- `node-name`: nome leg√≠vel do node
- `state`: EXECUTED, IDLE, CONFIGURED
- `model`: configura√ß√µes espec√≠ficas do node (varia por tipo)
- `nodeAnnotation`: anota√ß√£o/coment√°rio do usu√°rio

### IMPLEMENTA√á√ÉO

```python
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List
from pathlib import Path
import xml.etree.ElementTree as ET

@dataclass
class NodeSettings:
    """Configura√ß√µes extra√≠das do settings.xml de um node."""
    factory: str
    node_name: str
    state: str
    annotation: str = ""
    model: Dict[str, Any] = field(default_factory=dict)
    
    # Campos calculados
    node_type: str = ""  # Categoria inferida do factory
    is_loop_start: bool = False
    is_loop_end: bool = False
    is_variable_node: bool = False

class SettingsParser:
    """
    Parser para arquivos settings.xml de nodes KNIME.
    Extrai factory, configura√ß√µes e modelo do node.
    """
    
    NAMESPACE = {"k": "http://www.knime.org/2008/09/XMLConfig"}
    
    # Padr√µes para identificar tipos especiais de nodes
    LOOP_START_PATTERNS = ["LoopStart", "GroupLoopStart", "ChunkLoopStart"]
    LOOP_END_PATTERNS = ["LoopEnd", "VariableLoopEnd"]
    VARIABLE_NODE_PATTERNS = ["Variable", "FlowVariable", "TableRow"]
    
    def __init__(self, settings_path: Path):
        """
        Args:
            settings_path: Caminho para arquivo settings.xml
        """
        pass
    
    def parse(self) -> NodeSettings:
        """
        Faz parsing do settings.xml.
        
        Returns:
            NodeSettings com todas as informa√ß√µes extra√≠das
        """
        pass
    
    def _extract_factory(self, root: ET.Element) -> str:
        """Extrai a classe factory do node."""
        pass
    
    def _extract_node_name(self, root: ET.Element) -> str:
        """Extrai o nome leg√≠vel do node."""
        pass
    
    def _extract_state(self, root: ET.Element) -> str:
        """Extrai o estado de execu√ß√£o."""
        pass
    
    def _extract_annotation(self, root: ET.Element) -> str:
        """Extrai a anota√ß√£o/coment√°rio do node."""
        pass
    
    def _extract_model(self, root: ET.Element) -> Dict[str, Any]:
        """
        Extrai o modelo/configura√ß√µes do node.
        
        O modelo √© espec√≠fico de cada tipo de node.
        Esta fun√ß√£o extrai recursivamente todos os valores.
        """
        pass
    
    def _parse_config_recursive(self, element: ET.Element) -> Dict[str, Any]:
        """
        Parse recursivo de elementos <config> e <entry>.
        
        Converte:
        - <entry type="xstring" value="X"> -> str
        - <entry type="xint" value="N"> -> int
        - <entry type="xdouble" value="N.N"> -> float
        - <entry type="xboolean" value="true"> -> bool
        - <config> com filhos -> dict recursivo
        - <entry> com array-size -> list
        """
        pass
    
    def _infer_node_type(self, factory: str) -> str:
        """
        Infere categoria do node pelo factory.
        
        Categorias: io, manipulation, transformation, logic, 
                    flow_control, database, other
        """
        pass
    
    def _check_special_patterns(self, factory: str, node_name: str) -> Dict[str, bool]:
        """Verifica se √© loop start/end ou variable node."""
        pass
    
    def get_model_value(self, path: str, default: Any = None) -> Any:
        """
        Obt√©m valor do modelo por caminho.
        
        Exemplo: get_model_value("column-filter/included_names")
        
        Args:
            path: Caminho separado por "/" 
            default: Valor se n√£o encontrar
        """
        pass
```

### TRATAMENTO DE TIPOS XML

O KNIME usa tipos espec√≠ficos no atributo `type`:
- `xstring` ‚Üí str
- `xint` ‚Üí int
- `xlong` ‚Üí int
- `xdouble` ‚Üí float
- `xboolean` ‚Üí bool (converte "true"/"false")
- `xchar` ‚Üí str (√∫nico caractere)

### ARQUIVO: tests/test_settings_parser.py

Crie testes com XMLs de exemplo para diferentes tipos de nodes:

1. CSV Reader (io)
2. Column Filter (manipulation)  
3. Math Formula (transformation)
4. Rule Engine (logic)
5. Group Loop Start (flow_control)

Para cada um, teste:
- Extra√ß√£o do factory
- Extra√ß√£o do model com valores corretos
- Infer√™ncia correta do node_type
- Detec√ß√£o de patterns especiais (loop, variable)

### XML DE TESTE - CSV Reader

```xml
<?xml version="1.0" encoding="UTF-8"?>
<config xmlns="http://www.knime.org/2008/09/XMLConfig" key="settings.xml">
    <entry key="factory" type="xstring" value="org.knime.base.node.io.csvreader.CSVReaderNodeFactory"/>
    <entry key="node-name" type="xstring" value="CSV Reader"/>
    <entry key="state" type="xstring" value="EXECUTED"/>
    <config key="model">
        <entry key="url" type="xstring" value="/data/input.csv"/>
        <entry key="colDelimiter" type="xstring" value=","/>
        <entry key="hasColHeader" type="xboolean" value="true"/>
        <entry key="hasRowHeader" type="xboolean" value="false"/>
    </config>
    <config key="nodeAnnotation">
        <entry key="text" type="xstring" value="L√™ arquivo de contratos"/>
    </config>
</config>
```

### XML DE TESTE - Column Filter

```xml
<?xml version="1.0" encoding="UTF-8"?>
<config xmlns="http://www.knime.org/2008/09/XMLConfig" key="settings.xml">
    <entry key="factory" type="xstring" value="org.knime.base.node.preproc.filter.column.DataColumnSpecFilterNodeFactory"/>
    <entry key="node-name" type="xstring" value="Column Filter"/>
    <entry key="state" type="xstring" value="EXECUTED"/>
    <config key="model">
        <config key="column-filter">
            <entry key="filter-type" type="xstring" value="STANDARD"/>
            <config key="included_names">
                <entry key="array-size" type="xint" value="3"/>
                <entry key="0" type="xstring" value="NuContrato"/>
                <entry key="1" type="xstring" value="NmProduto"/>
                <entry key="2" type="xstring" value="VrContrato"/>
            </config>
            <config key="excluded_names">
                <entry key="array-size" type="xint" value="1"/>
                <entry key="0" type="xstring" value="TempColumn"/>
            </config>
        </config>
    </config>
</config>
```

Implemente c√≥digo completo com todos os m√©todos funcionais.
```

---

### PROMPT 1.4 - Parser do spec.xml

```
Adicione ao projeto existente um parser para arquivos spec.xml (schema de sa√≠da dos nodes).

## ARQUIVO: src/extractors/spec_parser.py

Crie uma classe `SpecParser` que extrai o schema de sa√≠da de cada porta de um node.

### CONTEXTO T√âCNICO

Nodes executados t√™m pastas `port_N/` contendo:
- `spec.xml`: schema das colunas de sa√≠da
- `data.xml`: dados em si (opcional, pode ser bin√°rio)

O spec.xml cont√©m:
- `number_columns`: quantidade de colunas
- `column_spec_N`: especifica√ß√£o de cada coluna
  - `column_name`: nome da coluna
  - `column_type/cell_class`: tipo KNIME da coluna
  - `column_domain`: dom√≠nio (min/max, valores poss√≠veis)

### IMPLEMENTA√á√ÉO

```python
from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List
from pathlib import Path
import xml.etree.ElementTree as ET

@dataclass
class ColumnSpec:
    """Especifica√ß√£o de uma coluna."""
    name: str
    knime_type: str
    python_type: str
    domain: Dict[str, Any] = field(default_factory=dict)
    
@dataclass  
class PortSpec:
    """Especifica√ß√£o de uma porta de sa√≠da."""
    port_index: int
    columns: List[ColumnSpec]
    row_count: Optional[int] = None
    
    @property
    def column_count(self) -> int:
        return len(self.columns)
    
    @property
    def column_names(self) -> List[str]:
        return [c.name for c in self.columns]

class SpecParser:
    """
    Parser para arquivos spec.xml de portas de sa√≠da KNIME.
    Extrai schema de colunas com nomes, tipos e dom√≠nios.
    """
    
    NAMESPACE = {"k": "http://www.knime.org/2008/09/XMLConfig"}
    
    # Mapeamento de tipos KNIME para Python/Pandas
    TYPE_MAPPING = {
        # Num√©ricos
        "org.knime.core.data.def.DoubleCell": "Float64",
        "org.knime.core.data.def.IntCell": "Int64",
        "org.knime.core.data.def.LongCell": "Int64",
        "org.knime.core.data.DoubleValue": "Float64",
        "org.knime.core.data.IntValue": "Int64",
        "org.knime.core.data.LongValue": "Int64",
        
        # String
        "org.knime.core.data.def.StringCell": "str",
        "org.knime.core.data.StringValue": "str",
        
        # Boolean
        "org.knime.core.data.def.BooleanCell": "bool",
        "org.knime.core.data.BooleanValue": "bool",
        
        # Data/Tempo
        "org.knime.core.data.date.DateAndTimeCell": "datetime64[ns]",
        "org.knime.core.data.date.DateAndTimeValue": "datetime64[ns]",
        "org.knime.core.data.time.localdate.LocalDateCell": "datetime64[ns]",
        "org.knime.core.data.v2.time.LocalDateTimeValueFactory": "datetime64[ns]",
        "org.knime.core.data.v2.time.LocalDateValueFactory": "datetime64[ns]",
        
        # Cole√ß√µes e outros
        "org.knime.core.data.collection.ListCell": "object",
        "org.knime.core.data.collection.SetCell": "object",
        "org.knime.core.data.blob.BinaryObjectCell": "object",
        "org.knime.core.data.xml.XMLCell": "str",
    }
    
    def __init__(self, spec_path: Path):
        """
        Args:
            spec_path: Caminho para arquivo spec.xml
        """
        pass
    
    def parse(self) -> PortSpec:
        """
        Faz parsing do spec.xml.
        
        Returns:
            PortSpec com todas as colunas e seus tipos
        """
        pass
    
    def _extract_columns(self, root: ET.Element) -> List[ColumnSpec]:
        """Extrai especifica√ß√£o de todas as colunas."""
        pass
    
    def _extract_column_spec(self, column_config: ET.Element) -> ColumnSpec:
        """Extrai especifica√ß√£o de uma coluna."""
        pass
    
    def _extract_domain(self, domain_config: ET.Element) -> Dict[str, Any]:
        """
        Extrai dom√≠nio da coluna.
        
        Para num√©ricos: min, max
        Para strings: possible_values (se definido)
        """
        pass
    
    def _map_knime_type(self, knime_type: str) -> str:
        """
        Mapeia tipo KNIME para tipo Python/Pandas.
        
        Se tipo n√£o conhecido, tenta inferir por padr√µes no nome.
        Fallback: "object"
        """
        pass
    
    @classmethod
    def parse_node_ports(cls, node_folder: Path) -> Dict[int, PortSpec]:
        """
        Parseia todos os specs de portas de um node.
        
        Args:
            node_folder: Pasta do node
            
        Returns:
            Dict mapeando port_index -> PortSpec
        """
        pass

class SchemaRegistry:
    """
    Registro central de schemas para todos os nodes do workflow.
    Permite consultar schema de entrada/sa√≠da de qualquer node.
    """
    
    def __init__(self):
        self._schemas: Dict[str, Dict[int, PortSpec]] = {}
    
    def register(self, node_id: str, port_index: int, spec: PortSpec):
        """Registra schema de uma porta de um node."""
        pass
    
    def get_output_schema(self, node_id: str, port_index: int = 0) -> Optional[PortSpec]:
        """Obt√©m schema de sa√≠da de um node."""
        pass
    
    def get_input_schema(self, node_id: str, connections: List, port_index: int = 0) -> Optional[PortSpec]:
        """
        Obt√©m schema de entrada de um node.
        Baseado no schema de sa√≠da do node predecessor.
        """
        pass
    
    def to_dict(self) -> Dict:
        """Exporta todos os schemas como dict."""
        pass
```

### ARQUIVO: tests/test_spec_parser.py

Crie testes com spec.xml de exemplo:
- test_parse_returns_port_spec
- test_extract_columns_correct_count
- test_map_knime_type_double
- test_map_knime_type_string
- test_map_knime_type_date
- test_map_knime_type_unknown_fallback
- test_extract_domain_numeric
- test_extract_domain_categorical
- test_parse_node_ports_multiple

### XML DE TESTE

```xml
<?xml version="1.0" encoding="UTF-8"?>
<config xmlns="http://www.knime.org/2008/09/XMLConfig" key="spec.xml">
    <entry key="spec_name" type="xstring" value="default"/>
    <entry key="number_columns" type="xint" value="3"/>
    <config key="column_spec_0">
        <entry key="column_name" type="xstring" value="NuContrato"/>
        <config key="column_type">
            <entry key="cell_class" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
        </config>
        <config key="column_domain">
            <config key="lower_bound">
                <entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
                <config key="org.knime.core.data.def.DoubleCell">
                    <entry key="DoubleCell" type="xdouble" value="1000.0"/>
                </config>
            </config>
            <config key="upper_bound">
                <entry key="datacell" type="xstring" value="org.knime.core.data.def.DoubleCell"/>
                <config key="org.knime.core.data.def.DoubleCell">
                    <entry key="DoubleCell" type="xdouble" value="9999.0"/>
                </config>
            </config>
        </config>
    </config>
    <config key="column_spec_1">
        <entry key="column_name" type="xstring" value="NmProduto"/>
        <config key="column_type">
            <entry key="cell_class" type="xstring" value="org.knime.core.data.def.StringCell"/>
        </config>
        <config key="column_domain">
            <config key="possible_values">
                <entry key="array-size" type="xint" value="2"/>
                <config key="0">
                    <entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
                    <config key="org.knime.core.data.def.StringCell">
                        <entry key="StringCell" type="xstring" value="Ve√≠culos"/>
                    </config>
                </config>
                <config key="1">
                    <entry key="datacell" type="xstring" value="org.knime.core.data.def.StringCell"/>
                    <config key="org.knime.core.data.def.StringCell">
                        <entry key="StringCell" type="xstring" value="Im√≥veis"/>
                    </config>
                </config>
            </config>
        </config>
    </config>
    <config key="column_spec_2">
        <entry key="column_name" type="xstring" value="DtLiberacao"/>
        <config key="column_type">
            <entry key="cell_class" type="xstring" value="org.knime.core.data.time.localdate.LocalDateCell"/>
        </config>
        <config key="column_domain"/>
    </config>
</config>
```

Implemente c√≥digo completo com todos os m√©todos funcionais.
```

---

### PROMPT 1.5 - Expans√£o de Metanodes

```
Adicione ao projeto existente suporte para expans√£o recursiva de metanodes.

## CONTEXTO

Metanodes s√£o sub-workflows encapsulados. Cada metanode:
- √â identificado por `node_is_meta="true"` no workflow.knime
- Tem sua pr√≥pria pasta com workflow.knime interno
- Pode conter outros metanodes (recursivo)
- Exp√µe vari√°veis de fluxo para o workflow pai

## ARQUIVO: src/extractors/metanode_expander.py

```python
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
from pathlib import Path

@dataclass
class MetanodeInfo:
    """Informa√ß√µes de um metanode."""
    id: str
    name: str
    folder: Path
    parent_workflow: Optional[str] = None
    internal_workflow: Optional[Dict] = None
    exposed_variables: List[Dict[str, str]] = field(default_factory=list)
    depth: int = 0  # N√≠vel de aninhamento

class MetanodeExpander:
    """
    Expande metanodes recursivamente, parseando seus workflows internos.
    """
    
    # Padr√µes de metanodes especiais (data de refer√™ncia, etc)
    SPECIAL_METANODE_PATTERNS = [
        "CRIA DATA",
        "Data Ref",
        "DataReferencia", 
        "Data de Refer√™ncia"
    ]
    
    def __init__(self, workflow_root: Path, max_depth: int = 10):
        """
        Args:
            workflow_root: Pasta raiz do workflow principal
            max_depth: Profundidade m√°xima de recurs√£o (prote√ß√£o)
        """
        self.workflow_root = workflow_root
        self.max_depth = max_depth
        self._metanodes: Dict[str, MetanodeInfo] = {}
    
    def expand_all(self, nodes: Dict, connections: List) -> Dict[str, MetanodeInfo]:
        """
        Expande todos os metanodes encontrados.
        
        Args:
            nodes: Dict de nodes do workflow principal
            connections: Lista de conex√µes
            
        Returns:
            Dict mapeando node_id -> MetanodeInfo com workflow interno
        """
        pass
    
    def expand_metanode(self, node_id: str, node_info, depth: int = 0) -> MetanodeInfo:
        """
        Expande um metanode espec√≠fico.
        
        Args:
            node_id: ID do metanode
            node_info: NodeInfo do metanode
            depth: Profundidade atual de recurs√£o
            
        Returns:
            MetanodeInfo com workflow interno parseado
        """
        pass
    
    def _find_metanode_folder(self, node_info) -> Optional[Path]:
        """Encontra a pasta do metanode baseado no settings_path."""
        pass
    
    def _parse_internal_workflow(self, metanode_folder: Path) -> Dict:
        """
        Parseia o workflow interno do metanode.
        Usa os parsers existentes recursivamente.
        """
        pass
    
    def _extract_exposed_variables(self, metanode_folder: Path) -> List[Dict[str, str]]:
        """
        Extrai vari√°veis de fluxo expostas pelo metanode.
        
        Busca em:
        - workflow.knime -> flow_stack
        - Nodes internos do tipo Variable/FlowVariable
        """
        pass
    
    def _is_special_metanode(self, name: str) -> bool:
        """Verifica se √© um metanode especial (data de refer√™ncia, etc)."""
        pass
    
    def get_metanode(self, node_id: str) -> Optional[MetanodeInfo]:
        """Obt√©m metanode expandido por ID."""
        pass
    
    def get_all_nodes_flat(self) -> Dict[str, Any]:
        """
        Retorna todos os nodes (incluindo internos de metanodes) 
        em uma estrutura flat com IDs qualificados.
        
        Exemplo: metanode_10/node_5 para node 5 dentro do metanode 10
        """
        pass
    
    def to_dict(self) -> Dict:
        """Exporta todos os metanodes como dict serializ√°vel."""
        pass
```

## ARQUIVO: tests/test_metanode_expander.py

Crie testes:
- test_expand_all_finds_metanodes
- test_expand_metanode_parses_internal_workflow
- test_recursive_expansion_respects_max_depth
- test_extract_exposed_variables
- test_is_special_metanode_true
- test_is_special_metanode_false
- test_get_all_nodes_flat_includes_internal

## INTEGRA√á√ÉO

Atualize o `src/extractors/__init__.py` para exportar:
```python
from .zip_extractor import KnimeZipExtractor
from .workflow_parser import WorkflowParser, NodeInfo, ConnectionInfo
from .settings_parser import SettingsParser, NodeSettings
from .spec_parser import SpecParser, PortSpec, ColumnSpec, SchemaRegistry
from .metanode_expander import MetanodeExpander, MetanodeInfo
```

Implemente c√≥digo completo. O MetanodeExpander deve usar os parsers existentes (WorkflowParser, SettingsParser, SpecParser) internamente.
```

---

## üü° SPRINT 2: REPRESENTA√á√ÉO INTERMEDI√ÅRIA

### PROMPT 2.1 - Construtor de Grafo

```
Crie o m√≥dulo de constru√ß√£o de grafo de execu√ß√£o do workflow.

## ARQUIVO: src/ir/__init__.py

Crie arquivo vazio para o m√≥dulo.

## ARQUIVO: src/ir/graph_builder.py

```python
from dataclasses import dataclass, field
from typing import Dict, List, Set, Optional, Any, Tuple
from pathlib import Path
import json

# Opcional: usar networkx se dispon√≠vel, sen√£o implementar pr√≥prio
try:
    import networkx as nx
    HAS_NETWORKX = True
except ImportError:
    HAS_NETWORKX = False

@dataclass
class IRNode:
    """Representa√ß√£o intermedi√°ria de um node."""
    id: str
    name: str
    display_name: str
    factory: str
    category: str
    
    # Configura√ß√µes extra√≠das
    settings: Dict[str, Any] = field(default_factory=dict)
    annotation: str = ""
    
    # Estrutura
    is_metanode: bool = False
    is_loop_start: bool = False
    is_loop_end: bool = False
    is_variable_node: bool = False
    
    # Schema
    input_ports: List[Dict] = field(default_factory=list)
    output_ports: List[Dict] = field(default_factory=list)
    
    # Grafo
    predecessors: List[str] = field(default_factory=list)
    successors: List[str] = field(default_factory=list)
    
    # Processamento
    classification: str = "UNKNOWN"  # MAPPED, CANDIDATE, UNKNOWN
    python_code: str = ""
    
    # Metanode espec√≠fico
    internal_workflow: Optional[Dict] = None

@dataclass
class IREdge:
    """Representa√ß√£o intermedi√°ria de uma conex√£o."""
    id: str
    source_node: str
    target_node: str
    source_port: int
    target_port: int
    edge_type: str = "data"  # "data" ou "flow"

@dataclass
class WorkflowIR:
    """Representa√ß√£o intermedi√°ria completa do workflow."""
    name: str
    source_file: str
    
    # Metadados
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # Estrutura
    nodes: Dict[str, IRNode] = field(default_factory=dict)
    edges: List[IREdge] = field(default_factory=list)
    
    # Calculados
    execution_order: List[str] = field(default_factory=list)
    loops: List[Dict] = field(default_factory=list)
    
    # Estat√≠sticas
    statistics: Dict[str, Any] = field(default_factory=dict)

class GraphBuilder:
    """
    Constr√≥i grafo de execu√ß√£o a partir dos dados extra√≠dos.
    """
    
    def __init__(self):
        self._nodes: Dict[str, IRNode] = {}
        self._edges: List[IREdge] = []
        self._graph = None  # NetworkX DiGraph se dispon√≠vel
    
    def build(self, 
              parsed_workflow: Dict,
              node_settings: Dict[str, Any],
              node_specs: Dict[str, Any],
              metanodes: Dict[str, Any]) -> WorkflowIR:
        """
        Constr√≥i IR completo do workflow.
        
        Args:
            parsed_workflow: Resultado do WorkflowParser.parse()
            node_settings: Dict node_id -> NodeSettings parseados
            node_specs: Dict node_id -> PortSpec de cada node
            metanodes: Dict node_id -> MetanodeInfo expandidos
            
        Returns:
            WorkflowIR completo
        """
        pass
    
    def _create_ir_nodes(self, 
                         parsed_nodes: Dict,
                         settings: Dict,
                         specs: Dict,
                         metanodes: Dict) -> Dict[str, IRNode]:
        """Cria IRNode para cada node do workflow."""
        pass
    
    def _create_ir_edges(self, connections: List) -> List[IREdge]:
        """Cria IREdge para cada conex√£o."""
        pass
    
    def _build_adjacency(self):
        """Constr√≥i listas de predecessors/successors em cada node."""
        pass
    
    def _build_networkx_graph(self) -> Any:
        """Se networkx dispon√≠vel, cria DiGraph para an√°lises."""
        pass
    
    def _calculate_statistics(self) -> Dict[str, Any]:
        """Calcula estat√≠sticas do workflow."""
        pass
    
    def get_node(self, node_id: str) -> Optional[IRNode]:
        """Obt√©m node por ID."""
        return self._nodes.get(node_id)
    
    def get_predecessors(self, node_id: str) -> List[IRNode]:
        """Retorna nodes predecessores."""
        pass
    
    def get_successors(self, node_id: str) -> List[IRNode]:
        """Retorna nodes sucessores."""
        pass
    
    def get_roots(self) -> List[IRNode]:
        """Retorna nodes sem predecessores (in√≠cio do fluxo)."""
        pass
    
    def get_leaves(self) -> List[IRNode]:
        """Retorna nodes sem sucessores (fim do fluxo)."""
        pass
    
    def to_dict(self) -> Dict:
        """Exporta grafo como dict serializ√°vel."""
        pass
```

## ARQUIVO: tests/test_graph_builder.py

Crie testes:
- test_build_creates_workflow_ir
- test_create_ir_nodes_maps_all_nodes
- test_create_ir_edges_maps_all_connections
- test_build_adjacency_correct_predecessors
- test_build_adjacency_correct_successors
- test_get_roots_returns_source_nodes
- test_get_leaves_returns_sink_nodes
- test_to_dict_is_serializable

## ATUALIZA√á√ÉO: requirements.txt

Adicione:
```
networkx>=3.0  # Opcional mas recomendado
```

Implemente c√≥digo completo. Se networkx n√£o estiver dispon√≠vel, implemente as funcionalidades b√°sicas sem ele.
```

---

### PROMPT 2.2 - Ordena√ß√£o Topol√≥gica e Detec√ß√£o de Loops

```
Adicione ao projeto ordena√ß√£o topol√≥gica e detec√ß√£o de loops.

## ARQUIVO: src/ir/topological_sort.py

```python
from typing import Dict, List, Set, Tuple, Optional
from collections import deque
from dataclasses import dataclass

@dataclass
class LoopInfo:
    """Informa√ß√µes sobre um loop detectado."""
    id: str
    start_node: str
    end_node: str
    internal_nodes: List[str]
    loop_type: str  # "GroupLoop", "ChunkLoop", "GenericLoop"
    loop_variables: List[Dict[str, str]]

class TopologicalSorter:
    """
    Calcula ordem topol√≥gica de execu√ß√£o do workflow.
    Detecta e trata loops corretamente.
    """
    
    def __init__(self, nodes: Dict, edges: List):
        """
        Args:
            nodes: Dict node_id -> IRNode
            edges: Lista de IREdge
        """
        self.nodes = nodes
        self.edges = edges
        self._loops: List[LoopInfo] = []
        self._execution_order: List[str] = []
    
    def sort(self) -> Tuple[List[str], List[LoopInfo]]:
        """
        Calcula ordem topol√≥gica e detecta loops.
        
        Returns:
            Tuple de:
            - Lista de node_ids na ordem de execu√ß√£o
            - Lista de LoopInfo para loops detectados
            
        Raises:
            ValueError: Se houver ciclo n√£o tratado
        """
        pass
    
    def _detect_loops(self) -> List[LoopInfo]:
        """
        Detecta loops baseado em padr√µes de nodes.
        
        Procura pares de LoopStart/LoopEnd e identifica nodes internos.
        """
        pass
    
    def _find_loop_pairs(self) -> List[Tuple[str, str]]:
        """Encontra pares (start_id, end_id) de loops."""
        pass
    
    def _find_internal_nodes(self, start_id: str, end_id: str) -> List[str]:
        """
        Encontra nodes dentro de um loop.
        
        Nodes internos s√£o aqueles alcan√ß√°veis a partir do start
        e que alcan√ßam o end, sem sair do loop.
        """
        pass
    
    def _extract_loop_variables(self, start_id: str) -> List[Dict[str, str]]:
        """Extrai vari√°veis de loop do node start."""
        pass
    
    def _kahn_sort(self, 
                   nodes: Set[str], 
                   exclude_back_edges: Set[Tuple[str, str]] = None) -> List[str]:
        """
        Algoritmo de Kahn para ordena√ß√£o topol√≥gica.
        
        Args:
            nodes: Conjunto de node_ids a ordenar
            exclude_back_edges: Arestas de retorno de loops a ignorar
            
        Returns:
            Lista ordenada de node_ids
        """
        pass
    
    def _build_indegree(self, 
                        nodes: Set[str],
                        exclude_edges: Set[Tuple[str, str]] = None) -> Dict[str, int]:
        """Calcula in-degree de cada node."""
        pass
    
    def _build_adjacency_list(self,
                              nodes: Set[str],
                              exclude_edges: Set[Tuple[str, str]] = None) -> Dict[str, List[str]]:
        """Constr√≥i lista de adjac√™ncia."""
        pass
    
    def _handle_isolated_nodes(self, 
                               ordered: List[str], 
                               all_nodes: Set[str]) -> List[str]:
        """Adiciona nodes isolados (sem conex√µes) ao final."""
        pass
    
    def get_execution_order(self) -> List[str]:
        """Retorna ordem de execu√ß√£o calculada."""
        return self._execution_order
    
    def get_loops(self) -> List[LoopInfo]:
        """Retorna loops detectados."""
        return self._loops
    
    def validate_order(self) -> bool:
        """
        Valida se a ordem est√° correta.
        
        Para cada node, todos os predecessores devem vir antes.
        """
        pass
```

## ARQUIVO: tests/test_topological_sort.py

Crie testes:
- test_sort_simple_linear_workflow
- test_sort_workflow_with_branches
- test_sort_workflow_with_merge
- test_detect_loops_finds_group_loop
- test_find_internal_nodes_correct
- test_kahn_sort_respects_dependencies
- test_handle_isolated_nodes
- test_validate_order_true_for_valid
- test_validate_order_false_for_invalid

## CEN√ÅRIOS DE TESTE

### Workflow Linear
```
1 -> 2 -> 3 -> 4
Ordem esperada: [1, 2, 3, 4]
```

### Workflow com Branch
```
    -> 2 ->
1 ->       -> 4
    -> 3 ->
Ordem esperada: [1, 2, 3, 4] ou [1, 3, 2, 4]
```

### Workflow com Loop
```
1 -> 2(LoopStart) -> 3 -> 4(LoopEnd) -> 5
                     ^       |
                     +-------+

Loops: [{start: 2, end: 4, internal: [3]}]
Ordem: [1, 2, 3, 4, 5]
```

Implemente c√≥digo completo.
```

---

### PROMPT 2.3 - Exportador de IR para JSON

```
Adicione ao projeto o exportador de IR para JSON.

## ARQUIVO: src/ir/ir_exporter.py

```python
from typing import Dict, Any, Optional
from pathlib import Path
from datetime import datetime
import json

class IRExporter:
    """
    Exporta WorkflowIR para arquivo JSON estruturado.
    """
    
    def __init__(self, workflow_ir):
        """
        Args:
            workflow_ir: WorkflowIR a exportar
        """
        self.workflow_ir = workflow_ir
    
    def export(self, output_path: Path) -> Path:
        """
        Exporta IR para arquivo JSON.
        
        Args:
            output_path: Caminho do arquivo de sa√≠da
            
        Returns:
            Path do arquivo criado
        """
        pass
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Converte IR para dicion√°rio serializ√°vel.
        
        Estrutura:
        {
            "metadata": {...},
            "graph": {
                "nodes": {...},
                "edges": [...],
                "execution_order": [...],
                "loops": [...],
                "metanodes": {...}
            },
            "statistics": {...}
        }
        """
        pass
    
    def _serialize_metadata(self) -> Dict[str, Any]:
        """Serializa metadados do workflow."""
        pass
    
    def _serialize_nodes(self) -> Dict[str, Dict]:
        """Serializa todos os nodes."""
        pass
    
    def _serialize_node(self, node) -> Dict[str, Any]:
        """
        Serializa um IRNode.
        
        Inclui:
        - Identifica√ß√£o (id, name, factory)
        - Configura√ß√µes (settings, annotation)
        - Estrutura (is_metanode, is_loop_*)
        - Schema (input_ports, output_ports)
        - Grafo (predecessors, successors)
        - Processamento (classification, python_code)
        """
        pass
    
    def _serialize_edges(self) -> List[Dict]:
        """Serializa todas as arestas."""
        pass
    
    def _serialize_loops(self) -> List[Dict]:
        """Serializa informa√ß√µes de loops."""
        pass
    
    def _serialize_metanodes(self) -> Dict[str, Dict]:
        """Serializa metanodes com seus workflows internos."""
        pass
    
    def _calculate_statistics(self) -> Dict[str, Any]:
        """
        Calcula estat√≠sticas do IR:
        - total_nodes
        - total_edges
        - total_metanodes
        - total_loops
        - nodes_by_category
        - nodes_by_classification
        """
        pass
    
    @staticmethod
    def load(input_path: Path) -> Dict[str, Any]:
        """
        Carrega IR de arquivo JSON.
        
        Args:
            input_path: Caminho do arquivo
            
        Returns:
            Dict com IR carregado
        """
        pass
    
    @staticmethod
    def validate_ir(ir_dict: Dict) -> bool:
        """
        Valida estrutura do IR.
        
        Verifica:
        - Campos obrigat√≥rios presentes
        - Tipos corretos
        - Refer√™ncias v√°lidas (edges apontam para nodes existentes)
        """
        pass

class IRSummaryGenerator:
    """
    Gera resumo leg√≠vel do IR para logs e debugging.
    """
    
    def __init__(self, ir_dict: Dict):
        self.ir = ir_dict
    
    def generate_summary(self) -> str:
        """
        Gera resumo textual do workflow.
        
        Formato:
        ```
        WORKFLOW: nome_do_workflow
        Fonte: arquivo.knwf
        
        ESTAT√çSTICAS:
        - Nodes: 25 (18 MAPPED, 5 CANDIDATE, 2 UNKNOWN)
        - Edges: 24
        - Loops: 1
        - Metanodes: 3
        
        ORDEM DE EXECU√á√ÉO:
        1. CSV Reader (#1) [MAPPED]
        2. Column Filter (#2) [MAPPED]
        ...
        
        LOOPS:
        - Loop 1: nodes 5-8 (GroupLoop)
        
        NODES N√ÉO MAPEADOS:
        - Java Snippet (#22) [UNKNOWN]
        ```
        """
        pass
    
    def generate_node_table(self) -> str:
        """Gera tabela de nodes formatada."""
        pass
    
    def generate_coverage_report(self) -> str:
        """Gera relat√≥rio de cobertura."""
        pass
```

## ARQUIVO: tests/test_ir_exporter.py

Crie testes:
- test_export_creates_file
- test_to_dict_has_required_keys
- test_serialize_node_complete
- test_serialize_edges_correct_format
- test_load_returns_same_structure
- test_validate_ir_true_for_valid
- test_validate_ir_false_for_missing_fields
- test_summary_generator_readable_output

## ATUALIZA√á√ÉO: src/ir/__init__.py

```python
from .graph_builder import GraphBuilder, IRNode, IREdge, WorkflowIR
from .topological_sort import TopologicalSorter, LoopInfo
from .ir_exporter import IRExporter, IRSummaryGenerator
```

Implemente c√≥digo completo com encoding UTF-8 e formata√ß√£o JSON indentada.
```

---

## üî∂ CONTINUA NOS PR√ìXIMOS SPRINTS...

> **NOTA:** Os prompts dos Sprints 3 a 6 seguem o mesmo padr√£o detalhado.
> Por quest√µes de tamanho, veja o documento completo KNIME_TO_PYTHON_CONVERTER_ARCHITECTURE.md para a especifica√ß√£o completa de cada m√≥dulo.

---

## üìù NOTAS DE USO

### Ordem de Execu√ß√£o dos Prompts

1. **PROMPT ZERO** (em chat separado) - Extrair conhecimento do c√≥digo legado
2. Criar novo projeto e enviar prompts na ordem:
   - Sprint 1: 1.1 ‚Üí 1.2 ‚Üí 1.3 ‚Üí 1.4 ‚Üí 1.5
   - Sprint 2: 2.1 ‚Üí 2.2 ‚Üí 2.3
   - Sprint 3: 3.1 ‚Üí 3.2 ‚Üí 3.3
   - Sprint 4: 4.1 ‚Üí 4.2 ‚Üí 4.3
   - Sprint 5: 5.1
   - Sprint 6: 6.1 ‚Üí 6.2
   - Final: Orquestrador

### Valida√ß√£o entre Etapas

Ap√≥s cada prompt, execute:
```bash
# Verifica sintaxe
python -m py_compile src/modulo/arquivo.py

# Executa testes
pytest tests/test_arquivo.py -v
```

### Arquivos de Configura√ß√£o do Projeto Legado

Ap√≥s executar o PROMPT ZERO, copie os JSONs gerados para a pasta `config/` do novo projeto:
- `node_mapping_extracted.json` ‚Üí merge com `node_mapping.json`
- `knime_types_extracted.json` ‚Üí `knime_types.json`
- `connection_patterns_extracted.json` ‚Üí refer√™ncia para implementa√ß√£o

---

**Documento gerado em:** 2025-12-15  
**Vers√£o:** 1.0  
**Objetivo:** Guiar implementa√ß√£o no Windsurf com Claude Opus 4.5
